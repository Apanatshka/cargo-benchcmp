#!/usr/bin/env python

from __future__ import absolute_import, division, print_function

import argparse
from collections import namedtuple
import os.path as path
import re
import sys

try:
    strtype = unicode
except NameError:
    strtype = str


BENCHMARK_RE = re.compile(
    r'test\s+(?P<name>\S+)\s+... bench:\s+(?P<ns>[0-9,]+)\s+ns/iter\s+\(\+/-\s+(?P<variance>[0-9,]+)\)(?:\s+=\s+(?P<throughput>[0-9,]+))?')  # noqa


class Benchmark(namedtuple('_Benchmark', 'name ns variance throughput')):
    '''
    All extractable data from a single micro-benchmark.

    N.B. throughput may be None, but the rest of the fields are required.
    '''
    @classmethod
    def parse(cls, line):
        'Parses a single benchmark line into a Benchmark.'
        m = BENCHMARK_RE.match(line)
        if m is None:
            raise ValueError('could not parse benchmark in "%s"' % line)
        return Benchmark(name=m.group('name'),
                         ns=uncommafy(m.group('ns')),
                         variance=uncommafy(m.group('variance')),
                         throughput=uncommafy(m.group('throughput')))

    def compare(self, new):
        '''Compares an old benchmark (self) with a new benchmark.

        Returns a BenchmarkComparison.
        '''
        return Comparison(self, new)

    def format_ns(self, variance=False, throughput=True):
        fmt = ['%s' % commafy(self.ns)]
        if variance:
            fmt.append('(+/- %s)' % commafy(self.variance))
        if throughput and self.throughput is not None:
            fmt.append('(%s MB/s)' % commafy(self.throughput))
        return ' '.join(fmt)


class Comparison(namedtuple('Comparison', 'old new')):
    '''
    A comparison between an old and a new benchmark.

    All differences are reported in terms of measuring improvements
    (negative) or regressions (positive). That is, if an old benchmark
    is slower than a new benchmark, then the difference is negative.
    Conversely, if an old benchmark is faster than a new benchmark,
    then the difference is positive.
    '''

    @property
    def diff_ns(self):
        'Returns new.ns - old.ns'
        return self.new.ns - self.old.ns

    @property
    def diff_ratio(self):
        'Returns difference as a ratio of the old benchmark time.'
        return float(self.diff_ns) / float(self.old.ns)


def parse_benchmarks(all_benchmarks, strip=None):
    '''
    Parses a string containing all benchmarks.

    Returns an iterable of Benchmark.
    '''
    for line in re.split(r'\r?\n', all_benchmarks):
        line = line.strip()
        if len(line) == 0:
            continue
        try:
            b = Benchmark.parse(line)
            if strip is not None:
                b = b._replace(name=re.sub(strip, '', b.name))
            yield b
        except ValueError:
            # Couldn't parse line, probably fine to just skip along.
            continue


def uncommafy(s):
    if s is None:
        return s
    return int(s.replace(',', ''))


def commafy(num):
    num = [c for c in reversed(strtype(num))]
    with_commas = []
    while len(num) > 0:
        piece, num = num[:3], num[3:]
        with_commas.extend(piece)
        if len(piece) == 3 and len(num) > 0 and num[0] != '-':
            with_commas.append(',')
    return ''.join(reversed(with_commas))


def eprint(*args, **kwargs):
    kwargs['file'] = sys.stderr
    print(*args, **kwargs)


def format_table(lst, justify=None):
    '''
    Takes a list of iterables and returns them as a nicely formatted table.

    All values must be convertible to a strtype, or else a ValueError will
    be raised.
    '''
    pad = 2
    maxcols = []
    output = []
    first_row = True
    for row in lst:
        if row is None:
            output.append([])
            continue

        output_row = []
        for i, cell in enumerate(row):
            cell = strtype(cell)
            if first_row:
                maxcols.append(len(cell) + pad)
            else:
                maxcols[i] = max([maxcols[i], len(cell) + pad])
            output_row.append(cell)

        output.append(output_row)
        first_row = False

    justify = justify or (['l'] * len(maxcols))
    assert len(justify) == len(maxcols)
    nice = []
    for i, row in enumerate(output):
        nice_row = []
        for j, cell in enumerate(row):
            if justify[j] == 'l':
                nice_row.append(cell.ljust(maxcols[j]))
            else:
                nice_row.append(cell.rjust(maxcols[j]))
        nice.append(''.join(nice_row))
    return '\n'.join(nice)


def unique_names(old_fpath, new_fpath):
    ohead, otail = path.split(old_fpath)
    nhead, ntail = path.split(new_fpath)
    old, new = otail, ntail
    while ohead and otail and nhead and ntail:
        if old != new:
            return old, new
        ohead, otail = path.split(ohead)
        nhead, ntail = path.split(nhead)
        old, new = path.join(otail, old), path.join(ntail, new)
    return old, new


if __name__ == '__main__':
    p = argparse.ArgumentParser()
    p.add_argument('old', metavar='FILE')
    p.add_argument('new', metavar='FILE')
    p.add_argument('--threshold', type=float, default=None,
                   help='Only show comparisons with a percentage change '
                        'greater than this threshold.')
    p.add_argument('--regressions', action='store_true',
                   help='Only show regressions.')
    p.add_argument('--improvements', action='store_true',
                   help='Only show improvements.')
    p.add_argument('--variance', action='store_true', help='Show variance.')
    p.add_argument('--strip-old', default=None,
                   help='A regex to strip from old benchmark names.')
    p.add_argument('--strip-new', default=None,
                   help='A regex to strip from new benchmark names.')
    args = p.parse_args()

    with open(args.old) as f:
        olds = {b.name: b for b in parse_benchmarks(f.read(), args.strip_old)}
    with open(args.new) as f:
        news = {b.name: b for b in parse_benchmarks(f.read(), args.strip_new)}

    missed_old, missed_new = [], []
    old_name, new_name = unique_names(args.old, args.new)
    table = [[
        'name',
        '%s ns/iter' % old_name,
        '%s ns/iter' % new_name,
        'diff ns/iter',
        'diff %',
    ]]
    for name in sorted(olds):
        if name not in news:
            missed_old.append(olds[name])
            continue
        old, new = olds[name], news[name]
        comp = old.compare(new)
        per = comp.diff_ratio * 100
        if args.threshold is not None and abs(per) < args.threshold:
            continue
        if args.regressions and comp.diff_ns <= 0:
            continue
        if args.improvements and comp.diff_ns >= 0:
            continue
        table.append([
            name,
            old.format_ns(variance=args.variance),
            new.format_ns(variance=args.variance),
            '%s' % commafy(comp.diff_ns),
            '%0.2f%%' % per,
        ])
    for name in sorted(news):
        if name not in olds:
            missed_new.append(news[name])
    print(format_table(table, justify='lllrr'))
    if len(missed_old) > 0:
        eprint('WARNING: benchmarks present in old but not in new: %s'
               % ', '.join(b.name for b in missed_old))
    if len(missed_new) > 0:
        eprint('WARNING: benchmarks present in new but not in old: %s'
               % ', '.join(b.name for b in missed_new))
